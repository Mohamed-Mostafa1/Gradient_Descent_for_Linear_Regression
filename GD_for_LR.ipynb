{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c8286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd9d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(x, y, alpha, max_iterations):\n",
    "    theta0, theta1= 0, 0\n",
    "    # counter = 0\n",
    "    theta0_lst, theta1_lst, cost_lst, predicted_lst = [], [], [], []\n",
    "    no_samples = len(y)\n",
    "    for i in range(max_iterations):\n",
    "        # Calculating the predicted values\n",
    "        predicted = theta0 + theta1 * x\n",
    "        predicted_lst.append(predicted)\n",
    "        # Calculating the cost\n",
    "        cost = np.sum((predicted - y)**2) / (2 * no_samples)\n",
    "        cost_lst.append(cost)\n",
    "        # Calculating the gradient\n",
    "        grad0 = np.sum(predicted - y) / no_samples\n",
    "        grad1 = np.sum((predicted - y)*x) / no_samples\n",
    "        grad = np.array([grad0, grad1])\n",
    "        # Updating the thetas\n",
    "        theta0 = theta0 - (alpha * grad0)\n",
    "        theta1 = theta1 - (alpha * grad1) \n",
    "        theta0_lst.append(theta0)\n",
    "        theta1_lst.append(theta1)\n",
    "        #counter += 1\n",
    "        if len(cost_lst) > 1:\n",
    "            if norm(grad) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(cost_lst[i] - cost_lst[i-1]) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(norm([theta0_lst[i], theta1_lst[i]]) - norm([theta0_lst[i-1], theta1_lst[i-1]])) < 0.0001:\n",
    "                break\n",
    "    return theta0_lst[-1], theta1_lst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f48887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_grad_descent(x, y, alpha, max_iterations, batches):\n",
    "    theta0, theta1 = 0, 0\n",
    "    # counter = 0\n",
    "    theta0_lst, theta1_lst, cost_lst, predicted_lst = [], [], [], []\n",
    "    batch_samples = int(len(y) / batches)\n",
    "    for i in range(max_iterations):\n",
    "        for j in range(batch_samples):\n",
    "            # Calculating the predicted values\n",
    "            predicted = theta0 + theta1 * x\n",
    "            predicted_lst.append(predicted)\n",
    "            # Calculating the cost\n",
    "            cost = np.sum((predicted - y)**2) / (2 * batch_samples)\n",
    "            cost_lst.append(cost)\n",
    "            # Calculating the gradient\n",
    "            grad0 = np.sum(predicted - y) / batch_samples\n",
    "            grad1 = np.sum((predicted - y)*x) / batch_samples\n",
    "            grad = np.array([grad0, grad1])\n",
    "            # Updating the thetas\n",
    "            theta0 = theta0 - (alpha * sum(predicted - y) / batch_samples)\n",
    "            theta1 = theta1 - (alpha * sum((predicted - y) * x)) / (batch_samples) \n",
    "            theta0_lst.append(theta0)\n",
    "            theta1_lst.append(theta1)\n",
    "            # counter += 1\n",
    "         # Stop conditions\n",
    "        if len(cost_lst) > 1:\n",
    "            if norm(grad) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(cost_lst[i] - cost_lst[i-1]) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(norm([theta0_lst[i], theta1_lst[i]]) - norm([theta0_lst[i-1], theta1_lst[i-1]])) < 0.0001:\n",
    "                break\n",
    "    return theta0_lst[-1], theta1_lst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9f08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_grad_descent(x, y, alpha, max_iterations):\n",
    "    theta0, theta1 = 0, 0\n",
    "    # counter = 0\n",
    "    theta0_lst, theta1_lst, cost_lst, predicted_lst = [], [], [], []\n",
    "    no_samples = len(y)\n",
    "    for i in range(max_iterations):\n",
    "        for j in range(no_samples):\n",
    "            # Calculating the predicted values\n",
    "            predicted = theta0 + theta1 * x[j]\n",
    "            predicted_lst.append(predicted)\n",
    "            # Calculating the cost\n",
    "            cost = (predicted - y[j])**2 / 2 \n",
    "            cost_lst.append(cost)\n",
    "            # Calculating the gradient\n",
    "            grad0 = predicted - y[j]\n",
    "            grad1 = (predicted - y[j])*x[j]\n",
    "            grad=np.array([grad0, grad1])\n",
    "            # Updating the thetas\n",
    "            theta0 = theta0 - (alpha * grad0)\n",
    "            theta1 = theta1 - (alpha * grad1) \n",
    "            theta = np.array([theta0, theta1])\n",
    "            theta0_lst.append(theta0)\n",
    "            theta1_lst.append(theta1)\n",
    "            # counter += 1\n",
    "        # Stop conditions\n",
    "        if len(cost_lst) > 1:\n",
    "            if norm(grad) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(cost_lst[i] - cost_lst[i-1]) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(norm([theta0_lst[i], theta1_lst[i]]) - norm([theta0_lst[i-1], theta1_lst[i-1]])) < 0.0001:\n",
    "                break\n",
    "    return theta0_lst[-1], theta1_lst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13eb19cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(x, y, alpha, epsilon, max_iterations):\n",
    "    theta0, theta1, v0, v1 = 0, 0, 0, 0\n",
    "    # counter = 0\n",
    "    theta0_lst, theta1_lst, cost_lst, predicted_lst = [], [], [], []\n",
    "    no_samples = len(y)\n",
    "    for i in range(max_iterations):\n",
    "        # Calculating the predicted values\n",
    "        predicted = theta0 + theta1 * x\n",
    "        predicted_lst.append(predicted)\n",
    "        # Calculating error\n",
    "        err = predicted - y\n",
    "        # Calculating the cost\n",
    "        cost = np.sum(err**2) / (2 * no_samples)\n",
    "        cost_lst.append(cost)\n",
    "        # Calculating the gradient\n",
    "        grad0 = np.sum(err) / no_samples\n",
    "        grad1 = np.sum(err*x) / no_samples\n",
    "        grad = np.array([grad0, grad1])\n",
    "        # Calculating v for applying adaptive learning rate\n",
    "        v0 = v0 + (grad0**2)\n",
    "        v1 = v1 + (grad1**2)\n",
    "        # Calculating adaptive learning rate\n",
    "        alr0 = alpha / (np.sqrt(v0) + epsilon)\n",
    "        alr1 = alpha / (np.sqrt(v1) + epsilon)\n",
    "        # Updating the thetas\n",
    "        theta0 = theta0 - (alr0 * grad0)\n",
    "        theta1 = theta1 - (alr1 * grad1)\n",
    "        theta0_lst.append(theta0)\n",
    "        theta1_lst.append(theta1)\n",
    "        # counter += 1\n",
    "        if len(cost_lst) > 1:\n",
    "            if norm(grad) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(cost_lst[i] - cost_lst[i-1]) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(norm([theta0_lst[i], theta1_lst[i]]) - norm([theta0_lst[i-1], theta1_lst[i-1]])) < 0.0001:\n",
    "                break\n",
    "    return theta0_lst[-1], theta1_lst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe178cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprob(x, y, alpha, epsilon, beta, max_iterations):\n",
    "    theta0, theta1, v0, v1 = 0, 0, 0, 0\n",
    "    # counter = 0\n",
    "    theta0_lst, theta1_lst, cost_lst, predicted_lst = [], [], [], []\n",
    "    no_samples = len(y)\n",
    "    for i in range(max_iterations):\n",
    "        # Calculating the predicted values\n",
    "        predicted = theta0 + theta1 * x\n",
    "        predicted_lst.append(predicted)\n",
    "        # Calculating error\n",
    "        err = predicted - y\n",
    "        # Calculating the cost\n",
    "        cost = np.sum(err**2) / (2 * no_samples)\n",
    "        cost_lst.append(cost)\n",
    "        # Calculating the gradient\n",
    "        grad0 = np.sum(err) / no_samples\n",
    "        grad1 = np.sum(err*x) / no_samples\n",
    "        grad = np.array([grad0, grad1])\n",
    "        # Calculating v for applying adaptive learning rate\n",
    "        v0 = beta * v0 + (1-beta) * (grad0**2)\n",
    "        v1 = beta * v1 + (1 - beta) * (grad1**2)\n",
    "        # Calculating adaptive learning rate\n",
    "        alr0 = alpha / (np.sqrt(v0) + epsilon)\n",
    "        alr1 = alpha / (np.sqrt(v1) + epsilon)\n",
    "        # Updating the thetas\n",
    "        theta0 = theta0 - (alr0 * grad0)\n",
    "        theta1 = theta1 - (alr1 * grad1)\n",
    "        theta0_lst.append(theta0)\n",
    "        theta1_lst.append(theta1)\n",
    "        # counter += 1\n",
    "        if len(cost_lst) > 1:\n",
    "            if norm(grad) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(cost_lst[i] - cost_lst[i-1]) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(norm([theta0_lst[i], theta1_lst[i]]) - norm([theta0_lst[i-1], theta1_lst[i-1]])) < 0.0001:\n",
    "                break\n",
    "    return theta0_lst[-1], theta1_lst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56443c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(x, y, alpha, epsilon, beta1, beta2, max_iterations):\n",
    "    theta0, theta1, v0, v1, m0, m1 = 0, 0, 0, 0, 0, 0\n",
    "    # counter = 0\n",
    "    theta0_lst, theta1_lst, cost_lst, predicted_lst = [], [], [], []\n",
    "    no_samples = len(y)\n",
    "    for i in range(max_iterations):\n",
    "        # Calculating the predicted values\n",
    "        predicted = theta0 + theta1 * x\n",
    "        predicted_lst.append(predicted)\n",
    "        # Calculating error\n",
    "        err = predicted - y\n",
    "        # Calculating the cost\n",
    "        cost = np.sum(err**2) / (2 * no_samples)\n",
    "        cost_lst.append(cost)\n",
    "        # Calculating the gradient\n",
    "        grad0 = np.sum(err) / no_samples\n",
    "        grad1 = np.sum(err*x) / no_samples\n",
    "        grad = np.array([grad0, grad1])\n",
    "        # Calculating moment\n",
    "        m0 = beta1 * m0 + (1-beta1) * grad0\n",
    "        m1 = beta1 * m1 + (1-beta1) * grad1\n",
    "        # Calculating v for applying adaptive learning rate\n",
    "        v0 = beta2 * v0 + (1-beta2) * (grad0**2)\n",
    "        v1 = beta2 * v1 + (1-beta2) * (grad1**2)\n",
    "        # Bias Correction\n",
    "        m0_t = m0 / (1-(beta1**(i+1)))\n",
    "        m1_t = m1 / (1-(beta1**(i+1)))\n",
    "        \n",
    "        v0_t = v0 / (1-(beta2**(i+1)))\n",
    "        v1_t = v1 / (1-(beta2**(i+1)))\n",
    "        # Calculating adaptive learning rate\n",
    "        alr0 = alpha / (np.sqrt(v0_t) + epsilon)\n",
    "        alr1 = alpha / (np.sqrt(v1_t) + epsilon)\n",
    "        # Updating the thetas\n",
    "        theta0 = theta0 - (alr0 * m0_t)\n",
    "        theta1 = theta1 - (alr1 * m1_t)\n",
    "        theta0_lst.append(theta0)\n",
    "        theta1_lst.append(theta1)\n",
    "        # counter += 1\n",
    "        if len(cost_lst) > 1:\n",
    "            if norm(grad) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(cost_lst[i] - cost_lst[i-1]) < 0.0001:\n",
    "                break\n",
    "            elif np.absolute(norm([theta0_lst[i], theta1_lst[i]]) - norm([theta0_lst[i-1], theta1_lst[i-1]])) < 0.0001:\n",
    "                break\n",
    "    return theta0_lst[-1], theta1_lst[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
